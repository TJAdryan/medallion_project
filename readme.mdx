Medallion Architecture with GWAS Catalog Data ✨ 
This is a great example of an idea you might feel intimidated by before you realize what a simple concept it is, putting your data through more than one process in order to get a final result that is useable everytime.  
This project demonstrates the Medallion Architecture by building a data pipeline to process and transform public genetic data from the GWAS Catalog. This architecture is a design pattern used to organize data into logical layers as it moves through a data lake.

The pipeline processes data through three distinct layers, or two or four. Five is right out:

Bronze Layer: The raw, ingested data directly from the source.

Silver Layer: The cleaned, structured, and validated data.

Gold Layer: The aggregated, business-ready data optimized for analysis.

The Data: GWAS Catalog
We are using data from the GWAS Catalog maintained by the European Bioinformatics Institute (EBI) and the National Human Genome Research Institute (NHGRI). This catalog is a public database of published genome-wide association studies.

Source: https://www.ebi.ac.uk/gwas/api/search/downloads/alternative

Project Setup
1. Prerequisites
You'll need a Python environment with uv to manage packages.

Install uv: https://astral.sh/uv/install

2. Environment Setup
Create a dedicated virtual environment for the project.
```
uv venv
source .venv/bin/activate
```
3. Install Dependencies
We need a few key libraries for data manipulation and storage.

```uv install pandas pyarrow duckdb
``` 
Project Structure
The project is organized to reflect the layers of the Medallion Architecture.

```
.
├── .venv/                         # Virtual environment
├── data/
│   ├── bronze/                    # Raw, original data
│   ├── silver/                    # Cleaned and enriched data
│   └── gold/                      # Aggregated and finalized data
├── scripts/
│   ├── bronze_ingest.py           # Ingestion script
│   ├── silver_process.py          # Cleaning and structuring script
│   └── gold_transform.py          # Aggregation script
├── README.mdx                     # Project documentation
└── .gitignore                     # Git ignore file
```

The Pipeline Steps
Step 1: Bronze Layer - Data Ingestion
This step downloads the raw GWAS Catalog data. It is the entry point for our data.

scripts/bronze_ingest.py
```python
import requests
import os

url = "https://www.ebi.ac.uk/gwas/api/search/downloads/alternative"
filename = "gwas_catalog.tsv"
bronze_path = os.path.join("data", "bronze")
os.makedirs(bronze_path, exist_ok=True)
local_filepath = os.path.join(bronze_path, filename)

def download_file(url, local_filepath):
    if os.path.exists(local_filepath):
        print(f"File '{local_filepath}' already exists. Skipping download.")
        return
    print(f"Downloading {url} to {local_filepath}...")
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        with open(local_filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print("Download successful!")
    except requests.exceptions.RequestException as e:
        print(f"Error during download: {e}")

if __name__ == "__main__":
    download_file(url, local_filepath)

```
Step 2: Silver Layer - Data Processing & Cleaning
Here, we take the raw data, clean it, and structure it into a more usable format (Parquet).

scripts/silver_process.py
```python
import pandas as pd
import os
import pyarrow as pa
import pyarrow.parquet as pq

input_dir = os.path.join("data", "bronze")
output_dir = os.path.join("data", "silver")
os.makedirs(output_dir, exist_ok=True)
input_file = os.path.join(input_dir, "gwas_catalog.tsv")
output_file = os.path.join(output_dir, "gwas_catalog_clean.parquet")

def process_gwas_data():
    print("Reading raw data from the Bronze layer...")
    try:
        gwas_df = pd.read_csv(input_file, sep='\t')
    except FileNotFoundError:
        print(f"Error: Raw data file not found at {input_file}. Please ensure the file is in the 'data/bronze' directory.")
        return
    print("Cleaning and structuring the data...")
    cleaned_df = gwas_df[[
        'DATE ADDED TO CATALOG', 'JOURNAL', 'DISEASE/TRAIT', 'MAPPED_TRAIT',
        'SNPS', 'P-VALUE', 'OR or BETA'
    ]].copy()
    cleaned_df['DATE ADDED TO CATALOG'] = pd.to_datetime(cleaned_df['DATE ADDED TO CATALOG'])
    cleaned_df.rename(columns={
        'DATE ADDED TO CATALOG': 'date_added', 'JOURNAL': 'journal',
        'DISEASE/TRAIT': 'disease', 'MAPPED_TRAIT': 'mapped_trait',
        'SNPS': 'snps', 'P-VALUE': 'p_value', 'OR or BETA': 'or_or_beta'
    }, inplace=True)
    arrow_table = pa.Table.from_pandas(cleaned_df)
    print(f"Writing structured data to the Silver layer at {output_file}...")
    pq.write_table(arrow_table, output_file)
    print("Silver layer processing complete!")

if __name__ == "__main__":
    process_gwas_data()

```
Step 3: Gold Layer - Aggregation & Analysis
This is the final stage where we aggregate the clean data into a business-ready format and prepare it for analysis or visualization.

scripts/gold_transform.py   

```python
import pandas as pd
import os
import pyarrow as pa
import pyarrow.parquet as pq

silver_dir = os.path.join("data", "silver")
gold_dir = os.path.join("data", "gold")
os.makedirs(gold_dir, exist_ok=True)
input_file = os.path.join(silver_dir, "gwas_catalog_clean.parquet")
output_journal_file = os.path.join(gold_dir, "journal_study_counts.parquet")
output_disease_file = os.path.join(gold_dir, "disease_study_counts.parquet")

def aggregate_gwas_data():
    print("Reading data from the Silver layer...")
    try:
        clean_df = pd.read_parquet(input_file)
    except FileNotFoundError:
        print(f"Error: Silver layer file not found at {input_file}. Please run the silver_process.py script first.")
        return
    print("Aggregating study counts by journal...")
    journal_counts_df = clean_df.groupby('journal').size().reset_index(name='study_count')
    journal_counts_df.sort_values(by='study_count', ascending=False, inplace=True)
    journal_table = pa.Table.from_pandas(journal_counts_df)
    pq.write_table(journal_table, output_journal_file)
    print(f"Saved journal study counts to {output_journal_file}")
    print("Aggregating study counts by disease/trait...")
    disease_counts_df = clean_df.groupby('mapped_trait').size().reset_index(name='study_count')
    disease_counts_df.sort_values(by='study_count', ascending=False, inplace=True)
    disease_table = pa.Table.from_pandas(disease_counts_df)
    pq.write_table(disease_table, output_disease_file)
    print(f"Saved disease study counts to {output_disease_file}")
    print("\nGold layer processing complete! Final data is ready for analysis.")

if __name__ == "__main__":
    aggregate_gwas_data()
```

Step 4: Final Analysis & Visualization
Finally, you can use the Gold layer data to create charts for your blog post.

scripts/analyze_gold_data.py
```python
import pandas as pd
import os
import matplotlib.pyplot as plt

gold_dir = os.path.join("data", "gold")
input_file = os.path.join(gold_dir, "disease_study_counts.parquet")

def analyze_and_visualize():
    try:
        gold_df = pd.read_parquet(input_file)
        top_results = gold_df.head(10)
        print("\nTop 10 most studied diseases/traits:")
        print(top_results.to_string(index=False))

        top_results.plot(kind='bar', x='mapped_trait', y='study_count', legend=False)
        plt.title('Top 10 Most Studied Diseases/Traits')
        plt.xlabel('Disease/Trait')
        plt.ylabel('Study Count')
        plt.tight_layout()
        plt.show()

    except FileNotFoundError:
        print(f"Error: Gold layer file not found at {input_file}. Please run the gold_transform.py script first.")
        return

if __name__ == "__main__":
    analyze_and_visualize()
```
